<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Phonetics Recorder</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            padding: 20px;
            background: #f0f4f8;
            color: #333;
            line-height: 1.6;
        }
        #container {
            max-width: 800px;
            margin: 0 auto;
            background: #fff;
            padding: 20px;
            border-radius: 12px;
            box-shadow: 0 4px 12px rgba(0,0,0,0.08);
        }
        h1 {
            color: #1a73e8;
            text-align: center;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 20px;
        }
        th, td {
            padding: 12px;
            border: 1px solid #ddd;
            text-align: left;
        }
        th {
            background-color: #e9ecef;
        }
        .letter {
            font-weight: bold;
            font-size: 1.5em;
            text-align: center;
        }
        .phonetic-hint {
            font-style: italic;
            color: #555;
            text-align: center;
        }
        .status {
            font-size: 0.9em;
            text-align: center;
        }
        .status.saved {
            color: #28a745;
            font-weight: bold;
        }
        .status.recording {
            color: #dc3545;
            font-weight: bold;
        }
        button {
            margin-right: 5px;
            padding: 6px 10px;
            border-radius: 6px;
            border: 1px solid #ccc;
            background-color: #f7f7f7;
            cursor: pointer;
            transition: background-color 0.2s;
        }
        button:hover {
            background-color: #e9e9e9;
        }
        button:disabled {
            cursor: not-allowed;
            opacity: 0.6;
        }
        .record-btn { background-color: #007bff; color: white; }
        .stop-btn { background-color: #dc3545; color: white; }
        .play-btn { background-color: #28a745; color: white; }
        .delete-btn { background-color: #ffc107; color: #333; }

        /* Modal Styles */
        #edit-modal {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background-color: rgba(0, 0, 0, 0.5);
            display: flex;
            justify-content: center;
            align-items: center;
        }

        #edit-modal-content {
            background: #fff;
            padding: 20px;
            border-radius: 12px;
            box-shadow: 0 5px 15px rgba(0,0,0,0.3);
            width: 750px;
        }

        #waveform-canvas {
            border: 1px solid #ccc;
            cursor: grab;
        }
    </style>
</head>
<body>

    <div id="container">
        <h1>Phonetics Recorder</h1>
        <p>Use this tool to record custom audio for each letter's phonetic sound. The recordings will be saved in your browser. Click "Record", speak the sound, and then click "Stop".</p>
        
        <table>
            <thead>
                <tr>
                    <th>Letter</th>
                    <th>Phonetic Hint</th>
                    <th>Controls</th>
                    <th>Status</th>
                </tr>
            </thead>
            <tbody id="recorder-table-body">
                <!-- Rows will be generated by JavaScript -->
            </tbody>
        </table>

        <!-- Audio Editing Modal -->
        <div id="edit-modal" style="display: none;">
            <div id="edit-modal-content">
                <h2>Edit Recording</h2>
                <canvas id="waveform-canvas" width="700" height="150"></canvas>
                <div>
                    <button id="play-trim-btn">Play Trimmed</button>
                    <button id="save-trim-btn">Save Trimmed Audio</button>
                    <button id="cancel-edit-btn">Cancel</button>
                </div>
            </div>
        </div>
    </div>

    <script>
        document.addEventListener('DOMContentLoaded', () => {
            const PHONETIC_SOUNDS = {
                'a': 'ah', 'b': 'buh', 'c': 'kuh', 'd': 'duh', 'e': 'eh',
                'f': 'fuh', 'g': 'guh', 'h': 'huh', 'i': 'ih', 'j': 'juh',
                'k': 'kuh', 'l': 'luh', 'm': 'muh', 'n': 'nuh', 'o': 'oh',
                'p': 'puh', 'q': 'kwuh', 'r': 'ruh', 's': 'suh', 't': 'tuh',
                'u': 'uh', 'v': 'vuh', 'w': 'wuh', 'x': 'ksuh', 'y': 'yuh', 'z': 'zuh'
            };
            const ALPHABET = 'abcdefghijklmnopqrstuvwxyz'.split('');
            const tableBody = document.getElementById('recorder-table-body');

            let mediaRecorder;
            let audioChunks = [];
            let currentRecordingLetter = null;

            // 1. Generate the table
            function generateTable() {
                let html = '';
                for (const letter of ALPHABET) {
                    html += `
                        <tr data-letter="${letter}">
                            <td class="letter">${letter.toUpperCase()}</td>
                            <td class="phonetic-hint">"${PHONETIC_SOUNDS[letter]}"</td>
                            <td>
                                <button class="record-btn" data-action="record">Record</button>
                                <button class="play-btn" data-action="play" disabled>Play</button>
                                <button class="edit-btn" data-action="edit" disabled>Edit</button>
                                <button class="delete-btn" data-action="delete" disabled>Delete</button>
                            </td>
                            <td class="status">No Recording</td>
                        </tr>
                    `;
                }
                tableBody.innerHTML = html;
            }

            // 2. Check localStorage and update UI
            function loadInitialState() {
                for (const letter of ALPHABET) {
                    const storageKey = `phonetic_recording_${letter}`;
                    if (localStorage.getItem(storageKey)) {
                        updateUIAfterChange(letter, true);
                    }
                }
            }

            // 3. Handle all button clicks using event delegation
            tableBody.addEventListener('click', async (e) => {
                if (e.target.tagName !== 'BUTTON') return;

                const action = e.target.dataset.action;
                const row = e.target.closest('tr');
                const letter = row.dataset.letter;

                switch (action) {
                    case 'record':
                        await handleRecord(letter, e.target);
                        break;
                    case 'stop':
                        handleStop(letter, e.target);
                        break;
                    case 'play':
                        handlePlay(letter);
                        break;
                    case 'delete':
                        handleDelete(letter);
                        break;
                }
            });

            // 4. Recording Logic
            async function handleRecord(letter, recordButton) {
                if (mediaRecorder && mediaRecorder.state === 'recording') {
                    alert('Another recording is already in progress.');
                    return;
                }

                try {
                    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                    mediaRecorder = new MediaRecorder(stream);
                    currentRecordingLetter = letter;
                    audioChunks = [];

                    mediaRecorder.ondataavailable = event => {
                        audioChunks.push(event.data);
                    };

                    mediaRecorder.onstop = () => {
                        const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });
                        const reader = new FileReader();
                        reader.onloadend = () => {
                            const base64String = reader.result;
                            try {
                                localStorage.setItem(`phonetic_recording_${letter}`, base64String);
                                console.log(`Recording for '${letter}' saved.`);
                                updateUIAfterChange(letter, true);
                            } catch (error) {
                                console.error('Error saving to localStorage:', error);
                                alert('Could not save recording. Storage may be full.');
                                updateUIAfterChange(letter, false);
                            }
                        };
                        reader.readAsDataURL(audioBlob);
                        
                        // Clean up the stream
                        stream.getTracks().forEach(track => track.stop());
                    };

                    mediaRecorder.start();
                    updateRecordingUI(letter, true, recordButton);

                } catch (err) {
                    console.error("Error getting microphone access:", err);
                    alert("Microphone access is required to record. Please allow access and try again.");
                }
            }

            function handleStop(letter, stopButton) {
                if (mediaRecorder && mediaRecorder.state === 'recording') {
                    mediaRecorder.stop();
                    updateRecordingUI(letter, false, stopButton);
                }
            }

            // 5. Playback and Deletion Logic
            function handlePlay(letter) {
                const base64String = localStorage.getItem(`phonetic_recording_${letter}`);
                if (base64String) {
                    const audio = new Audio(base64String);
                    audio.play();
                } else {
                    alert('No recording found to play.');
                }
            }

            function handleDelete(letter) {
                if (confirm(`Are you sure you want to delete the recording for "${letter.toUpperCase()}"?`)) {
                    localStorage.removeItem(`phonetic_recording_${letter}`);
                    updateUIAfterChange(letter, false);
                    console.log(`Recording for '${letter}' deleted.`);
                }
            }

            // 6. UI Update Helpers
            function updateRecordingUI(letter, isRecording, button) {
                const row = tableBody.querySelector(`tr[data-letter="${letter}"]`);
                const statusCell = row.querySelector('.status');
                
                if (isRecording) {
                    statusCell.textContent = 'Recording...';
                    statusCell.className = 'status recording';
                    button.textContent = 'Stop';
                    button.dataset.action = 'stop';
                    button.className = 'stop-btn';
                } else {
                    statusCell.textContent = 'Processing...';
                    statusCell.className = 'status';
                    button.textContent = 'Record';
                    button.dataset.action = 'record';
                    button.className = 'record-btn';
                }
            }

            function updateUIAfterChange(letter, hasRecording) {
                const row = tableBody.querySelector(`tr[data-letter="${letter}"]`);
                const playBtn = row.querySelector('.play-btn');
                const editBtn = row.querySelector('.edit-btn');
                const deleteBtn = row.querySelector('.delete-btn');
                const statusCell = row.querySelector('.status');

                if (hasRecording) {
                    playBtn.disabled = false;
                    editBtn.disabled = false;
                    deleteBtn.disabled = false;
                    statusCell.textContent = 'Saved';
                    statusCell.className = 'status saved';
                } else {
                    playBtn.disabled = true;
                    editBtn.disabled = true;
                    deleteBtn.disabled = true;
                    statusCell.textContent = 'No Recording';
                    statusCell.className = 'status';
                }
            }

            // --- Initial Setup ---
            generateTable();
            loadInitialState();

            // --- Editing Logic ---
            const editModal = document.getElementById('edit-modal');
            const waveformCanvas = document.getElementById('waveform-canvas');
            const saveTrimBtn = document.getElementById('save-trim-btn');
            const cancelEditBtn = document.getElementById('cancel-edit-btn');
            const playTrimBtn = document.getElementById('play-trim-btn');
            const canvasCtx = waveformCanvas.getContext('2d');
            let currentAudioBuffer = null;
            let currentLetter = null;
            let trimStart = 0;
            let trimEnd = 1;

            tableBody.addEventListener('click', async (e) => {
                if (e.target.dataset.action === 'edit') {
                    currentLetter = e.target.closest('tr').dataset.letter;
                    const base64String = localStorage.getItem(`phonetic_recording_${currentLetter}`);
                    if (base64String) {
                        const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                        const response = await fetch(base64String);
                        const arrayBuffer = await response.arrayBuffer();
                        currentAudioBuffer = await audioContext.decodeAudioData(arrayBuffer);
                        drawWaveform(currentAudioBuffer);
                        editModal.style.display = 'flex';
                    }
                }
            });

            function drawWaveform(buffer) {
                const data = buffer.getChannelData(0);
                const width = waveformCanvas.width;
                const height = waveformCanvas.height;
                const step = Math.ceil(data.length / width);
                const amp = height / 2;

                canvasCtx.fillStyle = '#fff';
                canvasCtx.fillRect(0, 0, width, height);
                canvasCtx.strokeStyle = '#007bff';
                canvasCtx.lineWidth = 1;
                canvasCtx.beginPath();

                for (let i = 0; i < width; i++) {
                    let min = 1.0;
                    let max = -1.0;
                    for (let j = 0; j < step; j++) {
                        const datum = data[(i * step) + j];
                        if (datum < min) min = datum;
                        if (datum > max) max = datum;
                    }
                    canvasCtx.moveTo(i, (1 + min) * amp);
                    canvasCtx.lineTo(i, (1 + max) * amp);
                }
                canvasCtx.stroke();
                drawHandles();
            }

            function drawHandles() {
                const width = waveformCanvas.width;
                const height = waveformCanvas.height;
                const startX = trimStart * width;
                const endX = trimEnd * width;

                canvasCtx.fillStyle = 'rgba(255, 0, 0, 0.5)';
                canvasCtx.fillRect(startX, 0, 2, height);
                canvasCtx.fillRect(endX, 0, 2, height);
            }

            waveformCanvas.addEventListener('mousedown', (e) => {
                const rect = waveformCanvas.getBoundingClientRect();
                const x = e.clientX - rect.left;
                const width = waveformCanvas.width;
                const startX = trimStart * width;
                const endX = trimEnd * width;

                if (Math.abs(x - startX) < 5) {
                    waveformCanvas.onmousemove = (moveEvent) => {
                        const newX = moveEvent.clientX - rect.left;
                        trimStart = Math.max(0, Math.min(newX / width, trimEnd - 0.01));
                        drawWaveform(currentAudioBuffer);
                    };
                } else if (Math.abs(x - endX) < 5) {
                    waveformCanvas.onmousemove = (moveEvent) => {
                        const newX = moveEvent.clientX - rect.left;
                        trimEnd = Math.min(1, Math.max(newX / width, trimStart + 0.01));
                        drawWaveform(currentAudioBuffer);
                    };
                }

                waveformCanvas.onmouseup = () => {
                    waveformCanvas.onmousemove = null;
                };
            });

            saveTrimBtn.addEventListener('click', async () => {
                const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const start = Math.floor(trimStart * currentAudioBuffer.length);
                const end = Math.ceil(trimEnd * currentAudioBuffer.length);
                const newLength = end - start;
                const newBuffer = audioContext.createBuffer(1, newLength, currentAudioBuffer.sampleRate);
                currentAudioBuffer.copyFromChannel(newBuffer.getChannelData(0), 0, start);

                // Convert buffer to WAV and then to base64
                const newWav = bufferToWave(newBuffer, newLength);
                const reader = new FileReader();
                reader.onloadend = () => {
                    localStorage.setItem(`phonetic_recording_${currentLetter}`, reader.result);
                    editModal.style.display = 'none';
                };
                reader.readAsDataURL(new Blob([newWav], { type: 'audio/wav' }));
            });

            cancelEditBtn.addEventListener('click', () => {
                editModal.style.display = 'none';
            });

            playTrimBtn.addEventListener('click', () => {
                const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const start = Math.floor(trimStart * currentAudioBuffer.length);
                const end = Math.ceil(trimEnd * currentAudioBuffer.length);
                const newLength = end - start;
                const newBuffer = audioContext.createBuffer(1, newLength, currentAudioBuffer.sampleRate);
                currentAudioBuffer.copyFromChannel(newBuffer.getChannelData(0), 0, start);

                const source = audioContext.createBufferSource();
                source.buffer = newBuffer;
                source.connect(audioContext.destination);
                source.start();
            });

            // From https://github.com/mattdiamond/Recorderjs
            function bufferToWave(abuffer, len) {
                var numOfChan = abuffer.numberOfChannels, 
                    length = len * numOfChan * 2 + 44, 
                    buffer = new ArrayBuffer(length), 
                    view = new DataView(buffer), 
                    channels = [], i, sample, 
                    offset = 0, 
                    pos = 0;

                // write WAVE header
                setUint32(0x46464952);                         // "RIFF"
                setUint32(length - 8);                       // file length - 8
                setUint32(0x45564157);                         // "WAVE"

                setUint32(0x20746d66);                         // "fmt " chunk
                setUint32(16);                                 // length = 16
                setUint16(1);                                  // PCM (uncompressed)
                setUint16(numOfChan);
                setUint32(abuffer.sampleRate);
                setUint32(abuffer.sampleRate * 2 * numOfChan); // avg. bytes/sec
                setUint16(numOfChan * 2);                      // block-align
                setUint16(16);                                 // 16-bit

                setUint32(0x61746164);                         // "data" - chunk
                setUint32(length - pos - 4);                   // chunk length

                // write interleaved data
                for(i = 0; i < abuffer.numberOfChannels; i++)
                    channels.push(abuffer.getChannelData(i));

                while(pos < length) {
                    for(i = 0; i < numOfChan; i++) {             // interleave channels
                        sample = Math.max(-1, Math.min(1, channels[i][offset])); // clamp
                        sample = (0.5 + sample < 0 ? sample * 32768 : sample * 32767)|0; // scale to 16-bit signed int
                        view.setInt16(pos, sample, true);          // write 16-bit sample
                        pos += 2;
                    }
                    offset++                                     // next source sample
                }

                return buffer;

                function setUint16(data) {
                    view.setUint16(pos, data, true);
                    pos += 2;
                }

                function setUint32(data) {
                    view.setUint32(pos, data, true);
                    pos += 4;
                }
            }
        });
    </script>

</body>
</html>
